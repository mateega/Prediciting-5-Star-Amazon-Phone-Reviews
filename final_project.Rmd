---
title: "Modern Data Mining - Final Project: Predicting 5-star Amazon Phone Reviews"
author:
- Spencer Mateega
- Mehul Suri
- Aditya Maddipatla
date: 'April 30, 2023'
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, ISLR, rpart, rattle, pROC, partykit, ggplot2, glmnet, leaps, dplyr, keras, neuralnet, imager, ranger,tm, SnowballC, RColorBrewer, wordcloud, glmnet,
               randomForest, ranger, data.table, caret)
```

### Executive Summary
There were two types of models which were used: the random forest model and the LASSO regression model. In addition there were two variations of each model to check if dimensionality reduction increased accuracy: one which used the original dataframe and another which used the first 5 PCAs of the dataframe. The lasso model would be used as a basline model while the random forest would be used as the main model in order to form an accuracy comparision. We thought that the high dimensionality of the data combined with potential non-linear relationships would be better suited by a random forest model which better accounts for those features. Our findings confirmed this hypothesis as we saw that the random forests with and without PCA features outperformed their LASSO counterparts by around 10-15 percent. Additionlly, we found that the use of PCA did not improve accuracy, with the non-PCA models outperforming the PCA models. Overall, the best model was the random forest model using the standard, non-PCA dataframe as its input with an overall testing and validation error of around 6 percent.  

### Goal of the Study 

Surfing Amazon products and reviews to find which products are of the highest quality can often be a stressful and tedious task. We wanted to do a project that analyzes amazon products and helps a user understand which products are more likely to suit their needs based on the reviews of that product. Reviews are often lengthy and can also be misconstrued. The dataset we are using has over 414K rows of Amazon reviews about Unlocked Mobile Phones which will provide a healthy training set. Essentially, our goal is to use the text that comprises these reviews to predict whether a product was given a 5 star review or a sub 5 star review on Amazon by the user. This will be done by using text processing and then analyzing the performance of a variety of models that we learned about in class. We know that reviews on Amazon typically skew towards the higher end and having even a 4.5 star average rating can result in a poor product when you get it. However, more often than not a product that is 5 stars will satisfy a customer's expectations. Therefore, we thought predicting for 5 stars or not 5 stars makes the most sense. The overall flow of our product will be as following. First we will start by loading in and cleaning the dataset by removing nulls, filtering columns, etc. Next, we will perform EDA by analyzing important statistics about the dataset and displaying visuals that help us understand the dataset better. Next, we will move onto the actual prediction component of the project. We will split the data into train, test, and validation sets and then perform PCA analysis to reduce the dimensionality of the dataset. Next we will analyze the fit of several models to the data from what we learned in class. Our goal here is to find a model that best predicts on our data through the various model performance metrics that we use. In the end, we hope to have a tool that can be used by Amazon customers to save time and become more satisfied out of their shopping on Amazon.

### Data

We chose to use the Star Rating Prediction dataset from Kaggle to obtain Amazon reviews for Unlocked Mobile Phones. Included in the dataset, is the product name, brand name, price of the phone, review rating, the review itself, and the number of votes that the review got. The review rating ranges from 1 to 5 but as mentioned above we formed a new column for if a review was 5 star or not and will use that as our predictor label. Moreover, the dataset contains over 414K of reviews but in order to ensure that our models are about to train within a reasonable amount of time we decided to take phones from the top 20 brands and then chose a random subset of those reviews. We decided to do this reduction because of how long our models were taking to run originally on the entire set. The top 20 brands comprised over 90% of the phone reviews so this made logical sense. Moreover, we dropped all null rows. Now, our data was ready to analyze and work with!

### Findings


## Processing data

First, we load in the data from the Kaggle dataset.
```{r echo = T, results = 'hide'}
amazon <- read.csv("Amazon_Unlocked_Mobile.csv")
summary(amazon)
```
Next, we process the data. We see that we start with 414k rows and 6 columns. We have columns for product name, brand name, price, rating, reviews, and number of review votes. Rating is scored 1 to 5, reviews is a text review from a single user, and number of review votes is the number of votes that particular review recieved. Hence, each row represent an unique phone review. 

To process the data, we first drop the rows with N/A values. We then calcuate the number of ratings per unique brand and order the brands from most number of reviews to least number of reviews. We then slect only the top 20 reviews since our dataset starts with many random companies that we hadn't heard of. We then selct only the top 20 brands to leave us with brands with lots of reviews. This leaves us with companies such as Samsung, BLU, Apple, LG, BlackBerry, and Nokia. At this point we are still at over 300k rows. 

Given our data set is so large, we then take a random sample of 50k rows to move forward with for the rest of our project. This makes the run time of our future models reasonable. 

Our data is now cleaned.
```{r echo = T, results = 'hide'}
# number of rows and columns
nrow(amazon) # 413,840 rows
ncol(amazon) # 6 cols

# drop NA rows
amazon <- na.omit(amazon)

# calculate the number of ratings per brand and order by descending number of ratings
amazon_count <- amazon %>% 
  group_by(Brand.Name) %>% 
  summarize(num_ratings = n()) %>% 
  arrange(desc(num_ratings)) %>% 
  filter(Brand.Name != "")

# select only the top 20 brands
top_brands <- amazon_count$Brand.Name[1:20]
amazon_top <- amazon %>% 
  filter(Brand.Name %in% top_brands)

# number of rows and columns
nrow(amazon_top) # 307,826 rows

# take a random sample of 50k rows (our existing 400k rows make our models too slow)
set.seed(1)
amazon_sample <- amazon_top %>% 
  sample_n(50000, replace = FALSE)
amazon <- amazon_sample

# confirm the number of rows 
nrow(amazon) # 50,000 rows
```

## EDA

Next, we performed EDA on our data. 

We see the least priced phone is 1.73 USD and the most priced phone is 2,408.73 USD. The median priced phone is 139.95 USD and the average priced phone is 229.69 USD. 

We see most reviews don't receive any votes (median of 0) but some reviews have many votes (max of 478).

We also calculated the average rating by brand. We see brands have an average rating of 3.8 across their phones. The worst-rated brand (Polaroid) receives an average rating of 2.9 on their phones and the best-rated brand (OtterBox) receives an average rating of 4.5 on their phones.

We also calculate the number of ratings by bands. We see brands have a median number of ratings of 1,270 and an average number of ratings of 2,500. The least-rated brand (verykool) has 183 ratings and the most-rated brand (Samsung) has 10,372 ratings. 

We also look at the average phone price by brand. We see the minimum phone price is 90.39 USD, the median phone price is 247.30 USD, the mean phone price is 234.34 USD and the max phone price is 378.94 USD.

Looking at the distribution of the phone ratings, we see 17% of the reviews recieve 1 star, 6% of the reviews receive 2 stars, 8% of the ratings receive 3 stars, 15% of ratings recieve 4 stars and 55% of ratings recieve 5 stars.


```{r echo = T, results = 'hide'}
# price 
max(amazon$Price) # $2,408.73
min(amazon$Price) # $1.73
median(amazon$Price) # $139.95
mean(amazon$Price) # $229.69

# number of review votes
max(amazon$Review.Votes) # 478
min(amazon$Review.Votes) # 0 
median(amazon$Review.Votes) # 0 

# average rating by brand
amazon_avg <- amazon %>% 
  group_by(Brand.Name) %>% 
  summarize(avg_rating = mean(Rating), num_ratings = n()) %>% 
  arrange(num_ratings) %>% 
  mutate(Brand.Name = reorder(Brand.Name, num_ratings))

summary(amazon_avg)

ggplot(amazon_avg, aes(x=Brand.Name, y=avg_rating)) + 
  geom_bar(stat="identity", fill="blue") +
  labs(title="Average Rating per Brand", x="Brand", y="Average Rating") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# number of ratings by brand
ggplot(amazon_avg, aes(x = Brand.Name, y = num_ratings)) +
  geom_bar(stat = "identity") +
  xlab("Brand") +
  ylab("Number of ratings") +
  ggtitle("Number of ratings per brand") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# calculate the average price by brand
avg_price_by_brand <- amazon %>%
  group_by(Brand.Name) %>%
  summarize(avg_price = mean(Price)) %>%
  top_n(15, avg_price) # keep only the top 15 brands with the highest avg prices

summary(avg_price_by_brand)

# create a bar graph for average price by brand
ggplot(avg_price_by_brand, aes(x = Brand.Name, y = avg_price)) + 
  geom_bar(stat = "identity", fill = "blue") +
  xlab("Brand") +
  ylab("Average Price") +
  ggtitle("Brands vs Average Phone Prices") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# distribution of ratings
ggplot(amazon, aes(x=Rating)) + 
  geom_histogram(binwidth=1, fill="blue", color="white") +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +
  labs(title="Histogram of Ratings", x="Ratings", y="Frequency")

rating_dist <- amazon %>% count(amazon$Rating)
rating_dist$Percentage = (rating_dist$n / sum(rating_dist$n))*100
colnames(rating_dist)[colnames(rating_dist) == "amazon$Rating"] ="rating"
rating_dist$rating = factor(rating_dist$rating)

ggplot(rating_dist, aes(x = "", y = n, fill = rating)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  geom_text(aes(label = paste0(round(Percentage), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Pie Chart of Ratings")+
  scale_fill_manual(values = c("#F8766D", "#C49A00", "#53B400", "#00C094", "#00BFC4"))
```
## Creating a two-category response variable

To simplify our analysis, we create a two-category response variable to classify each review into either a 5-star rating or a non-5-star rating. We chose to go with 5-star vs non-5-star review over positive vs negative review because 5-star ratings make up over 50% of the reviews. 

We create a new column called "score" for this purpose.

```{r echo = T, results = 'hide'}
amazon$score <- c(0)
amazon$score[amazon$Rating == 5] <- 1

# distribution of scores
ggplot(amazon, aes(x=score)) + 
  geom_histogram(binwidth=1, fill="blue", color="white") +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +
  labs(title="Histogram of Scores", x="Scores", y="Frequency")

score_dist <- amazon %>% count(amazon$score)
score_dist$Percentage = (score_dist$n / sum(score_dist$n))*100
colnames(score_dist)[colnames(score_dist) == "amazon$score"] ="score"
score_dist$score = factor(score_dist$score)

ggplot(score_dist, aes(x = "", y = n, fill = score)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  geom_text(aes(label = paste0(round(Percentage), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Pie Chart of Ratings")+
  scale_fill_manual(values = c("#F8766D", "#00C094"))
```

## Document term matrix (aka bag of words)

Next, we extract a document term matrix for Reviews. This matrix represents a word frequencies matrix. For each review (rows), record the frequency of each word in the bag of words that appear in any of the reviews. We keep words appearing at least 0.5% of the time among all of the 50k documents. 

To clean the reviews we convert the text to lowercase, remove punctuation, remove numbers, and remove common English stopwords.

Finally, we combine our variable score with the dtm as a few data frame called amazon_df.

```{r echo = T, results = 'hide'}
# turn Reviews to corpus
corpus <- VCorpus(VectorSource(amazon$Reviews))

# control list for creating our DTM within DocumentTermMatrix
# we remove punction, numbers and stop words
control_list <- list( tolower = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = stopwords("english"),
stemming = TRUE)

# dtm with all terms:
dtm.long <- DocumentTermMatrix(corpus, control = control_list)

# kick out rare words
dtm<- removeSparseTerms(dtm.long, 1-.005)
inspect(dtm)

# create a new data frame
amazon_df <- data.frame(score = amazon$score, as.matrix(dtm))
```

## Analyses

Before we conduct our analysis, we split our data into training, testing, and validation data. We set aside 32,500 reviews for training, 12,500 reviews for testing, and 5,000 reviews for validation.

```{r echo = T, results = 'hide'}
set.seed(1)

# split data into training, testing, and validation data sets
index_train <- sample(1:nrow(amazon_df), 32500)
index_test <- sample(setdiff(1:nrow(amazon_df), index_train), 12500)
index_val <- setdiff(1:nrow(amazon_df), c(index_train, index_test))

#Factor the score column to make it categorical
amazon_df$score = factor(amazon_df$score)

# create training, testing, and validation data sets
amazon_df.train <- amazon_df[index_train, ]
amazon_df.test <- amazon_df[index_test, ]
amazon_df.val <- amazon_df[index_val, ]


```
### Lasso without PCA
We would like to start with a lasso model without using PCA. In this model we will train on all features to predict the "score" variable of the data frame. In addition, we will do validation testing on the lambda values to find the optimal lambda value. 
```{r}
#Run an iniitial glm model and find the optimal lambda value
y <- amazon_df.train$score
X1 <- sparse.model.matrix(score~., data=amazon_df.train)[, -1]
fit.lasso <- cv.glmnet(X1, y, alpha=.99, family="binomial")
lambda <- fit.lasso$lambda.min
plot(fit.lasso)
```
Using the optimal lambda value, we now create our final lasso model and predict on the test dataset, we find that our error is around 20%.
```{r}
# Create the final fit based on the optimal lambda value
final_fit.lasso <- glmnet(X1,y,alpha = 0.99,lambda=lambda,family="binomial")
predict.lasso <- predict(fit.lasso, as.matrix(amazon_df.test[,-1]), type = "class", s="lambda.1se")
# output majority vote labels
# LASSO testing errors
mean(amazon_df.test$score != predict.lasso)*100
```

### WORD CLOUD

From our LASSO model, we select the non-zero works picked up using lambda.1se. We select 328 words that are deemed useful.

We then feed the LASSO output from above to get a logistic regression, which we call fit.glm.

We then pull out all of the 5-star coefficients and the corresponding words. We want the coefficients in a decreasing order.

We then pick up the positive coefficients, which are positively related to the probablity of being a 5-star review. We see excelent and excel have the greatest coefficents (2.37 and 1.58 respectively) meaning these words have the highest probability of being associated with a 5-star review. Finally, we arrive at 145 good words which we associate with 5-star reviews.

We create a word cloud with the 100 leading good words, as shown below.

```{r}
# non-zero words picked up by LASSO when using lambda.1se
coef.1se <- coef(fit.lasso, s="lambda.1se")
lasso.words <- coef.1se@Dimnames[[1]] [coef.1se@i][-1] # non-zero variables without intercept.
summary(lasso.words)

# feed the output from LASSO above, get a logistic regression
sel_cols <- c("score", lasso.words)

# use all_of() to specify we would like to select variables in sel_cols
data_sub <- amazon_df.train %>% select(all_of(sel_cols))
fit.glm <- glm(score~., family=binomial, data_sub) # takes 3.5 minutes

# pull out all the positive coefficients and the corresponding words. Rank the coefficients in a decreasing order. 
fit.glm.coef <- coef(fit.glm)
hist(fit.glm.coef)

# pick up the positive coeffients
good.glm <- fit.glm.coef[which(fit.glm.coef > 0)]
good.glm <- good.glm[-1] # took intercept out
names(good.glm)[1:20] # which words are positively associated with good ratings
good.fre <- sort(good.glm, decreasing = TRUE) # sort the coef's
length(good.fre) # 145 good words
hist(as.matrix(good.fre), breaks=30, col="red")
good.word <- names(good.fre) # good words with a decreasing order in the coeff's

# word cloud
cor.special <- brewer.pal(8,"Dark2") # set color scheme
wordcloud(good.word[1:100], good.fre[1:100], colors=cor.special, ordered.colors=F)
```
## PCA
Now we would like to generate PCA versions of the training and test datasets as an alternative method. We will do every modle with and without PCA to see which of the models perform the best. 

As we will realize later, though, the models without using PCA seem to perform better than the models with using PCA.
```{r}
#Create the pcas for the amazon df
pca <- prcomp(amazon_df.train[,-1], scale. = TRUE)
pve <- summary(pca)$importance[2, 1:30]
plot(pve, type="b", pch = 19, frame = FALSE)
```
We see that 5 is acceptable choice of PCAs to choose at it is close to being the "elbow" of the variance graph. 
```{r}
#Create pca-versions of the training and test dataset
num_pcas <- 5
pcs = pca$x[,1:num_pcas]
pca_train_df = data.frame(score = amazon_df.train$score, pcs)
pcs_test <- predict(pca,newdata = as.matrix(amazon_df.test[,-1]))[,c(1:num_pcas)]
pca_test_df <- data.frame(pcs_test)
pcs_valid <- predict(pca,newdata = as.matrix(amazon_df.val[,-1]))[,c(1:num_pcas)]
pca_valid_df <- data.frame(pcs_valid)
```
## LASSO Model with PCA
We would now like to test the LASSO model using the PCA version of the inputs. We would like to see if the model can benefit from dimensionality reduction, however after running the model we see that accuracy is around 32.8 percent, which is less than the accuracy of the model without using PCA.
```{r}
#Creates the x and y components lasso model to predict a score, find the best lambda value: use PCA of x and y train
y <- amazon_df.train$score
X1 <- sparse.model.matrix(score~., data=pca_train_df)[, -1]
pca_fit.lasso <- cv.glmnet(X1, y, alpha=.99, family="binomial")
lambda <- pca_fit.lasso$lambda.min
plot(fit.lasso)
```
```{r}
#Create a new model using the best lambda value
pca_final_fit.lasso <- glmnet(X1,y,alpha = 0.99,lambda=lambda,family="binomial")
pca_predict.lasso <- predict(pca_final_fit.lasso, as.matrix(pca_test_df), type = "class", s="lambda.1se")
# output majority vote labels
# LASSO testing errors
mean(amazon_df.test$score != pca_predict.lasso)*100
```

## Random Forest Classification

We would now like to predict on the dataset with the random forest model. It is a non-linear model which may perform better on this dataset with its many dimensions.

We will create both the models with and without PCA using 200 trees and prediciting using all features apart from score in the dataset. 

In addition, the Gini index for classification will be used as the measure for variable importance--we are able to predict a classification model due to our factoring of the score variable during the data processing step. 

Random Forest without PCA
```{r}
#Create a Random forest model based on the pca values of train data--use PCA
fit.rf <- ranger::ranger(score~., amazon_df, num.trees = 200, importance="impurity") # no plot
fit.rf
```

```{r}
#Find the testing error of the Random Forest
predict.rf <- predict(fit.rf, data=amazon_df.test, type="response") # output the classes by majority
mean(amazon_df.test$score != predict.rf$predictions)*100
```

Random Forest with PCA
```{r}
#Create a Random forest model based on the pca values of train data--use PCA
pca_fit.rf <- ranger::ranger(score~., pca_train_df, num.trees = 200, importance="impurity") # no plot
pca_fit.rf
```
```{r}
#Find the testing error of the Random Forest
pca_predict.rf <- predict(pca_fit.rf, data=pca_test_df, type="response") # output the classes by majority
mean(amazon_df.test$score != pca_predict.rf$predictions)*100
```
Once again, we see that the model without the PCA outperformed the model with PCA. Additionally, we have seen a considerable increase in accuracy from the lasso models most likely due to the high dimensionality of the dataset. 

## Model Results
Out of the four models which were implemented, the random forest model without using PCA performed the best on the testing data set, to help confirm this conclusion all four models will run on the validation dataset and their respective accuracies will be compared 
```{r}
#Find the validation error of all four models
#LASSO without PCA
val_predict.lasso <- predict(final_fit.lasso, as.matrix(amazon_df.val[,-1]), type = "class", s="lambda.1se")
#LASSO with PCA
val_pca_predict.lasso <- predict(pca_final_fit.lasso, as.matrix(pca_valid_df), type = "class", s="lambda.1se")
#Random Forest without PCA
val_predict.rf <- predict(fit.rf, data=amazon_df.val, type="response") 
#Random Forest with PCA
val_pca_predict.rf <- predict(pca_fit.rf, data=pca_valid_df, type="response")

# LASSO without PCA error
mean(amazon_df.val$score != val_predict.lasso)*100
#LASSO with PCA error
mean(amazon_df.val$score != val_pca_predict.lasso)*100
#Random Forest without PCA error
mean(amazon_df.val$score != val_predict.rf$predictions)*100
#Random Forest with PCA error
mean(amazon_df.val$score != val_pca_predict.rf$predictions)*100
```
The order of the validation errors is:
1. LASSO without PCA
2. LASSO with PCA
3. Random Forest without PCA
4. Random Forest with PCA

Within these results, we see once again that the non-PCA models outperformed the PCA models and that the best model is the random forest model without PCA--which had a validation error of 5.92 percent. 

## Conclusion
The best model achieved would be the random forest model with standard inputs which acheived around 94 percent (94.08) accuracy on the validation set. We now display a confusion matrix on the final model's prediction on the validation set and we see that both the false positives and false negative misclassification errors are low as well. Overall, this model will be useful to predict 5 star phone products on Amazon and help Amazon users become more informed consumers. 
```{r}
table(val_predict.rf$predictions,amazon_df.val$score)
```